datasets:
  train:
    human5k:
      fasta: data/train/human5k/sequences.fasta
      embeddings:
        prott5: data/train/human/prott5_embeddings.h5
        esm2: data/train/human/esm2_embeddings.h5
      gemme:
        raw: data/train/human5k/gemme_predictions/
        processed: data/train/human5k/gemme_predictions.h5
      splits:
        train: data/train/human/splits/train_full.csv
        val: data/train/human/splits/val_full.csv
    droso4k:
      fasta: data/train/droso/sequences.fasta
      embeddings:
        prott5: data/train/droso/prott5_embeddings.h5
        esm2: data/train/droso/esm2_embeddings.h5
      gemme:
        raw: data/train/droso4k/gemme_predictions/
        processed: data/train/droso4k/gemme_predictions.h5
      splits:
        train: data/train/droso/splits/train_full.csv
        val: data/train/droso/splits/val_full.csv
    ecoli2k:
      fasta: data/train/ecoli/sequences.fasta
      embeddings:
        prott5: data/train/ecoli/prott5_embeddings.h5
        esm2: data/train/ecoli/esm2_embeddings.h5
      gemme:
        raw: data/train/ecoli2k/gemme_predictions/
        processed: data/train/ecoli2k/gemme_predictions.h5
      splits:
        train: data/train/ecoli/splits/train_full.csv
        val: data/train/ecoli/splits/val_full.csv
    virus1k:
      fasta: data/train/virus1k/sequences.fasta
      embeddings:
        prott5: data/train/virus1k/prott5_embeddings.h5
        esm2: data/train/virus1k/esm2_embeddings.h5
      gemme:
        raw: data/train/virus1k/gemme_predictions/
        processed: data/train/virus1k/gemme_predictions.h5
      splits:
        train: data/train/virus1k/splits/train_full.csv
        val: data/train/virus1k/splits/val_full.csv
    all9k:
      fasta: data/train/all/sequences.fasta
      embeddings:
        prott5: data/train/all/prott5_embeddings.h5
        esm2: data/train/all/esm2_embeddings.h5
      gemme:
        raw: data/train/all9k/gemme_predictions/
        processed: data/train/all9k/gemme_predictions.h5
      splits:
        train: data/train/all/splits/train_full.csv
        val: data/train/all/splits/val_full.csv
  test:
    proteingym87:
      fasta: data/test/proteingym87/unique_proteins.fasta
    proteingym217:
      fasta: data/test/proteingym217/proteingym_217.fasta
    megadataset146:
      fasta: data/test/stabilitydenovo146/mega_denovo_146_sequences.fasta

gemme:
  alphabet: "ACDEFGHIKLMNPQRSTVWY"

random:
  seed: 42

preprocessing:
  na_threshold: .5

splits:
  validation_size: 0.2

training:
  single_taxon:
    sampling_strategy: basic
  all_taxa:
    sampling_strategy: basic

eval:
  batch_size: 8192
  proteingym:
    reference_file:
      v1: https://raw.githubusercontent.com/OATML-Markslab/Tranception/main/proteingym/ProteinGym_reference_file_substitutions.csv
      v2: https://raw.githubusercontent.com/OATML-Markslab/ProteinGym/main/reference_files/DMS_substitutions.csv
    dms_files: https://marks.hms.harvard.edu/proteingym/DMS_ProteinGym_substitutions.zip

models:
  fnn:
    architecture: fnn
    model_parameters:
      hidden_dims: [256, 64]
      dropout_rate: 0.2
    training_parameters:
      learning_rate: 0.0001
      batch_size:
        training: 25000
        validation: 8192
      epochs: 200
      val_every_epoch: 1
      checkpoint_every_epoch: ~

  fnn_1_layer:
    architecture: fnn
    model_parameters:
      hidden_dims: [256]
      dropout_rate: 0.2
    training_parameters:
      learning_rate: 0.0001
      batch_size:
        training: 25000
        validation: 8192
      epochs: 200
      val_every_epoch: 1
      checkpoint_every_epoch: ~

  linreg:
    architecture: fnn
    model_parameters:
      hidden_dims: []
      dropout_rate: 0.2
    training_parameters:
      learning_rate: 0.0001
      batch_size:
        training: 25000
        validation: 8192
      epochs: 200
      val_every_epoch: 1
      checkpoint_every_epoch: ~

  cnn:
    architecture: cnn
    model_parameters:
      n_channels: 256
      kernel_size: 7
      padding: 3
      fully_connected_layers: [ 256, 64 ]
      dropout:
        fnn: 0.2
        cnn: 0.2
    training_parameters:
      learning_rate: 0.0001
      batch_size:
        training: 25000
        validation: 8192
      epochs: 200
      val_every_epoch: 1
      checkpoint_every_epoch: ~

wandb:
  username: <username>
  project: <project>
